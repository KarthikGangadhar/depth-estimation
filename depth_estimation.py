# -*- coding: utf-8 -*-
"""Copy of finalProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eLEdcHTECZZNcU6DY5LWZJzUvGnzy7Mh

Mahin's compressed data file
"""

# !wget -cq https://7cwlzw.bn.files.1drv.com/y4mWAvQpGUbcoxt7W3n_WjrKLU83U-pA7Kxgzqf3-WGPVqNGsIpVmTWxgJ0QTrR5zim8qWLznmGSVZVO3opMGdCiVohrsahq1gJ2qaaj54lHtbOYhFUo3gJDJ-Vsoa9L1twvaZOF5FPlPY-sJKr2XiXsC8BA2DmEV_h5-P62pNviMB-cO4W0dVEds9b2_F6iNpSGyjRUJL-gv1H0F5YSEOtguzOrnFD65fh3RVbh8e6-rE/nyu_data.zip

"""Yash's Reduced Data"""

# !wget -cp https://4iz2la.bn.files.1drv.com/y4mbkdtKoJnq_NBglCJJGvfrQdi3_-t48BOwi5p_lk_RA_R40AE2vZrXkhxo68LU6CXBI9Prxwle01FTupuJPexObohquqHgRiv1-oHAndwkJjPcoMaM3ALvTMn9go-BLYs7ehAb3tOfVCOnaHdGrtJcIQf8Zs6O82r4PzbbLJVWzsBF-Du_xtC3DutBA-ADiNHN2XZu1M8jOyRztXQxbNgbolbvmAfy9mbb_3WWN5SimM/nyu_data.zip
"""Reduced train file and main file, change the batch size, epoch and learning rate and run this file to start the training"""

import os, sys, glob, time, pathlib

from keras import applications
from keras.models import Model, load_model
from keras.layers import Input, InputLayer, Conv2D, Activation, LeakyReLU, Concatenate
from keras.optimizers import Adam

# Kerasa / TensorFlow
from helpers import BilinearUpSampling2D
from utilities import get_nyu_train_test_data, load_test_data, depth_loss_function

# Define upsampling layer
def upproject(base_model, tensor, filters, name, concat_with):
    up_i = BilinearUpSampling2D((2, 2), name=name+'_upsampling2d')(tensor)
    up_i = Concatenate(name=name+'_concat')([up_i, base_model.get_layer(concat_with).output]) # Skip connection
    up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')(up_i)
    up_i = LeakyReLU(alpha=0.2)(up_i)
    up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')(up_i)
    up_i = LeakyReLU(alpha=0.2)(up_i)
    return up_i

def create_model():
    
    # Encoder Layers
    # print('Loading base model (DenseNet)..')
    # base_model = applications.DenseNet121(input_shape=(None, None, 3), include_top=False, weights='imagenet')
    
    # print('Base model loaded.')

    # # Starting point for decoder
    # base_model_output_shape = base_model.layers[-1].output.shape
    # print(" Output shape is: ", base_model_output_shape)

    # # Layer freezing?
    # for layer in base_model.layers: layer.trainable = True
    
    # # Starting half number of decoder filters
    
    # decode_filters = int(int(base_model_output_shape[-1])/2)

    # decoder = Conv2D(filters=decode_filters, kernel_size=1, padding='same', input_shape=base_model_output_shape, name='conv2')(base_model.output)
    # decoder = upproject(base_model, decoder, int(decode_filters/2), 'up1', concat_with='pool3_pool')
    # decoder = upproject(base_model, decoder, int(decode_filters/4), 'up2', concat_with='pool2_pool')
    # decoder = upproject(base_model, decoder, int(decode_filters/8), 'up3', concat_with='pool1')
    # decoder = upproject(base_model, decoder, int(decode_filters/16), 'up4', concat_with='conv1/relu')
    # if False: decoder = upproject(base_model, decoder, int(decode_filters/32), 'up5', concat_with='input_1')
    
    # # Extract depths (final layer)
    # conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')(decoder)
    
    # # Create the model
    # model = Model(inputs=base_model.input, outputs=conv3)
    # print('Model created.')
    net_input = Input(shape=(240,320,3))
    base_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=net_input)

    for layer in base_model.layers[:17]:
        layer.trainable = False
    
    x = base_model.layers[-2].output
    
    ### Decoder
    x = Deconv2D(256, (3,3), strides=(2,2), activation='relu', padding='same')(x)
    x = Deconv2D(128, (3,3), strides=(2,2), activation='relu', padding='same')(x)
    x = Deconv2D(64, (3,3), strides=(2,2), activation='relu', padding='same')(x)
    x = Deconv2D(32, (3,3), strides=(2,2), activation='relu', padding='same')(x)
    x = Deconv2D(1, (1,1), activation='sigmoid', padding='same')(x)
    
    # return model
    return (base_model,x)

def train(args, batch_size = 5 , epochs = 5, lr = 0.0001):
    batch_size = int(float(args[0]))
    epochs = int(float(args[1])) 
    lr = float(args[2])
    print("batch_size = {0} , epochs = {1}, lr = {2}".format(batch_size,epochs, lr))

    #creates encoder and decoder model
    base_model , model = create_model()

    train_generator, test_generator = get_nyu_train_test_data( batch_size )

    # Training session details
    runPath = os.path.join(os.getcwd(),'models',str(int(time.time())))
    pathlib.Path(runPath).mkdir(parents=True, exist_ok=True)
    print('Output: ' + runPath)

    # Optimizer
    # optimizer = Adam(lr=lr, amsgrad=True)
    optimizer = keras.optimizers.RMSprop(lr=5e-4)

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    print('Ready for training!\n')

    # Start training
    model.fit_generator(train_generator, callbacks=None, validation_data=test_generator, epochs=epochs, shuffle=True)

    # Save the final trained model:
    base_model.save('base_model.h5')
    print("model save completed!!!")
    print(model.history())

if __name__ == "__main__":
    args = sys.argv[ 1: ]
    if (len(args) <= 0) : 
        sys.exit( 0 )
    
    batch_size = int(args[0]), 
    epochs = int(args[1]), 
    lr = float(args[2]),
    train(args, batch_size, epochs, lr)